{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсинг сообщений из вашего личного архива VK\n",
    "\n",
    "В архиве вашего профиля ВК можно найти папку `messages`, где каждая переписка вынесена в отдельную папку. Чтобы понять какую именно папку открывать, заглянем в `index-messages.html` и найдём нужного человека. Перейдя по ссылке, в адресной строке вы увидете что-то типа:\n",
    "```\n",
    "file:///home/username/.fr-GZFN5j/messages/-209008242/messages0.html\n",
    "```\n",
    "Здесь `-209008242` именно тот номер, который нас интересует. Находим эту папку в `messages` и переносим её в папку users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4 as bs4\n",
    "import os\n",
    "\n",
    "USERS_DIR = os.path.join(os.getcwd(), \"users\")\n",
    "\n",
    "\n",
    "def get_user_message_files(user_dir_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of all html files for a given user.\"\n",
    "\n",
    "    :param user_dir_name: name of the user folder\n",
    "    :return: list of all files in the directory corresponding to this user in the USERS_DIR folder\n",
    "    \"\"\"\n",
    "    messages_files = []\n",
    "    user_dir = os.path.join(USERS_DIR, user_dir_name)\n",
    "    for file_name in os.listdir(user_dir):\n",
    "        if file_name.endswith(\".html\"):\n",
    "            messages_files.append(os.path.join(user_dir, file_name))\n",
    "    return messages_files\n",
    "\n",
    "\n",
    "def parse_messages(html_code, user_name) -> str:\n",
    "    \"\"\"\n",
    "    Extracts all text messages from an HTML document with correspondence.\n",
    "    Forwarded messages, pictures, and other attachments are ignored.\n",
    "    Each new paragraph in the message is saved on a separate line.\n",
    "\n",
    "    :param html_code: a string containing the HTML code to extract messages from\n",
    "    :return: a string containing the extracted messages\n",
    "    \"\"\"\n",
    "    messages_text = \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    message_divs = soup.find_all('div', class_='message')\n",
    "\n",
    "    for message_div in reversed(message_divs):\n",
    "        kludges_div = message_div.find('div', class_='kludges')\n",
    "        if kludges_div:\n",
    "            parent_div = kludges_div.find_parent('div')\n",
    "\n",
    "            name_div = message_div.find('div', class_='message__header')\n",
    "            name_a = name_div.find('a')\n",
    "            if name_a:\n",
    "                name = name_a.text.strip()\n",
    "            else:\n",
    "                name = user_name\n",
    "            for content in parent_div:\n",
    "                if type(content) == bs4.element.NavigableString:\n",
    "                    messages_text += f\"{name}:{content}\\n\"\n",
    "    return messages_text\n",
    "\n",
    "\n",
    "def write_user_messages_to_file(user_dir_name, user_name, output_file):\n",
    "    \"\"\"\n",
    "     Extracts the text of all messages for the user and writes them to the output file.\n",
    "\n",
    "    :param user_dir_name: the ID of the user whose messages will be processed\n",
    "    :param output_file: the file path where the text of all messages will be written to\n",
    "    \"\"\"\n",
    "\n",
    "    messages_files = get_user_message_files(user_dir_name)\n",
    "    sorted_files = sorted(messages_files, reverse=True)  # Sort the files in descending order\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for file in sorted_files:\n",
    "            with open(os.path.join(USERS_DIR, user_dir_name, file), 'r', encoding='windows-1251') as html_file:\n",
    "                messages_text = parse_messages(html_file.read(), user_name)\n",
    "                f.write(messages_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Находим сообщения, содержащие нужные слова\n",
    "\n",
    "Сначала используем парсер и выгрузим сообщения в переменную `messages_text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = \"!!!ENTER DIALOG ID!!!\"\n",
    "SELF_NAME = \"Иван Петров\" # ENTER YOUR NAME\n",
    "\n",
    "write_user_messages_to_file(USER, SELF_NAME, \"output.txt\")\n",
    "f = open(\"output.txt\", \"r\") \n",
    "messages_text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно начинать искать слова"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Способ №1\n",
    "\n",
    "Делаем всё по умному, используем специальную библиотеку обработки естественного языка.   \n",
    "Я не до конца разобрался в том, как она работает, так что если будете использовать этот метод, то рекомендую создать какую-нибудь тестовую строку и попробовать найти нужное слово в ней. После этого должно стать понятнее какие строки нужно вписать в `if stemmed_toked in (...)` и `if token.endswith(...):`. В данном примере я хотел найти все вхождения слова \"люблю\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вхождений слова 'Люблю': 1382\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "text = messages_text\n",
    "stemmer = RussianStemmer()\n",
    "\n",
    "tokens = word_tokenize(text, language=\"russian\")\n",
    "\n",
    "words_count = 0\n",
    "for token in tokens:\n",
    "    stemmed_token = stemmer.stem(token)\n",
    "\n",
    "    if stemmed_token in (\"люб\", \"любл\"):\n",
    "\n",
    "        if token.endswith(\"лю\"):\n",
    "            words_count += 1\n",
    "\n",
    "print(\"Вхождений слова Люблю:\", words_count)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Способ №2\n",
    "\n",
    "Берём из файла с перепиской только те строчки, в которых встречается нужное нам слово. Аномалии в результате можно будет найти глазами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'r', encoding='utf-8') as input_file, \\\n",
    "     open('messages.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for line in input_file:\n",
    "        if 'люблю' in line.lower():\n",
    "            output_file.write(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
